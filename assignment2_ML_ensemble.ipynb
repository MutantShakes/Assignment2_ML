{"cells":[{"cell_type":"markdown","metadata":{},"source":["<h1> Ensemble Models</h1>"]},{"cell_type":"markdown","metadata":{},"source":["<h2>Stacking</h2>"]},{"cell_type":"markdown","metadata":{},"source":["**1. Stacking (random_forest, gradient_boosting, xgboost), --with Date Column Dropped--**\n","\n","**-- Stacking Ensemble Model Accuracy: 0.9183728936401522**"]},{"cell_type":"code","execution_count":162,"metadata":{"execution":{"iopub.execute_input":"2023-12-16T15:10:02.650023Z","iopub.status.busy":"2023-12-16T15:10:02.648901Z","iopub.status.idle":"2023-12-16T15:10:02.658681Z","shell.execute_reply":"2023-12-16T15:10:02.657662Z","shell.execute_reply.started":"2023-12-16T15:10:02.649976Z"},"trusted":true},"outputs":[],"source":["# # Import necessary libraries\n","# from sklearn.model_selection import train_test_split\n","# from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n","# from sklearn.svm import SVC\n","# from sklearn.ensemble import StackingClassifier\n","# from sklearn.metrics import accuracy_score\n","# from sklearn.datasets import make_classification\n","# from xgboost import XGBClassifier\n","\n","# # Generate a toy dataset\n","# # X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n","\n","# # Split the dataset into training and testing sets\n","# train_X, val_X, train_y, val_y = train_test_split(train_features, train_labels)\n","\n","# # Define individual models\n","# model_rf = RandomForestClassifier(n_estimators=100, random_state=42)\n","# model_gb = GradientBoostingClassifier(n_estimators=100, random_state=42)\n","# model_xgb = XGBClassifier(random_state=42)\n","\n","# # Define the stacking ensemble model\n","# estimators = [('rf', model_rf), ('gb', model_gb), ('xgb', model_xgb)]\n","# stacking_model = StackingClassifier(estimators=estimators, final_estimator=RandomForestClassifier(random_state=42))\n","\n","# # Train the stacking ensemble model\n","# stacking_model.fit(train_features, train_labels)\n","\n","# # Make predictions on the test set\n","# stacking_pred = stacking_model.predict(test_features)\n","\n","# # Calculate accuracy of the stacking ensemble model\n","# stacking_model.fit(train_X,train_y)\n","# pred_y = stacking_model.predict(val_X) \n","# stacking_accuracy = accuracy_score(val_y, pred_y)\n","# print(f\"Stacking Ensemble Model Accuracy: {stacking_accuracy}\")\n","\n","# ----------------------------------------------------\n","# Stacking Ensemble Model Accuracy: 0.9183728936401522\n","# ----------------------------------------------------"]},{"cell_type":"markdown","metadata":{},"source":["**2. Stacking (random_forest, gradient_boosting, xgboost), --WithOUT Date Column Dropped--**\n","\n","**-- Stacking Ensemble Model Accuracy: 0.9128012321072658**"]},{"cell_type":"code","execution_count":163,"metadata":{"execution":{"iopub.execute_input":"2023-12-16T15:10:02.661319Z","iopub.status.busy":"2023-12-16T15:10:02.660881Z","iopub.status.idle":"2023-12-16T15:10:02.677187Z","shell.execute_reply":"2023-12-16T15:10:02.676050Z","shell.execute_reply.started":"2023-12-16T15:10:02.661285Z"},"trusted":true},"outputs":[],"source":["# # Import necessary libraries\n","# from sklearn.model_selection import train_test_split\n","# from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n","# from sklearn.svm import SVC\n","# from sklearn.ensemble import StackingClassifier\n","# from sklearn.metrics import accuracy_score\n","# from sklearn.datasets import make_classification\n","# from xgboost import XGBClassifier\n","\n","# # Generate a toy dataset\n","# # X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n","\n","# # Split the dataset into training and testing sets\n","# train_X, val_X, train_y, val_y = train_test_split(train_features, train_labels)\n","\n","# # Define individual models\n","# model_rf = RandomForestClassifier(n_estimators=100, random_state=42)\n","# model_gb = GradientBoostingClassifier(n_estimators=100, random_state=42)\n","# model_xgb = XGBClassifier(random_state=42)\n","\n","# # Define the stacking ensemble model\n","# estimators = [('rf', model_rf), ('gb', model_gb), ('xgb', model_xgb)]\n","# stacking_model = StackingClassifier(estimators=estimators, final_estimator=RandomForestClassifier(random_state=42))\n","\n","# # Train the stacking ensemble model\n","# stacking_model.fit(train_features, train_labels)\n","\n","# # Make predictions on the test set\n","# stacking_pred = stacking_model.predict(test_features)\n","\n","# # Calculate accuracy of the stacking ensemble model\n","# stacking_model.fit(train_X,train_y)\n","# pred_y = stacking_model.predict(val_X) \n","# stacking_accuracy = accuracy_score(val_y, pred_y)\n","# print(f\"Stacking Ensemble Model Accuracy: {stacking_accuracy}\")\n","\n","# -----------------------------------------------------\n","# Stacking Ensemble Model Accuracy: 0.9128012321072658\n","# -----------------------------------------------------\n"]},{"cell_type":"markdown","metadata":{},"source":["**3.  Stacking (random_forest, gradient_boosting, xgboost), --With XGB hyperparameters tuned--**\n","\n","**-- Stacking Ensemble Model Accuracy: 0.9174216343540497**"]},{"cell_type":"code","execution_count":164,"metadata":{"execution":{"iopub.execute_input":"2023-12-16T15:10:02.680524Z","iopub.status.busy":"2023-12-16T15:10:02.679089Z","iopub.status.idle":"2023-12-16T15:10:02.695931Z","shell.execute_reply":"2023-12-16T15:10:02.694854Z","shell.execute_reply.started":"2023-12-16T15:10:02.680479Z"},"trusted":true},"outputs":[],"source":["# # Import necessary libraries\n","# from sklearn.model_selection import train_test_split\n","# from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n","# from sklearn.svm import SVC\n","# from sklearn.ensemble import StackingClassifier\n","# from sklearn.metrics import accuracy_score\n","# from sklearn.datasets import make_classification\n","# from xgboost import XGBClassifier\n","\n","# # Generate a toy dataset\n","# # X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n","\n","# # Split the dataset into training and testing sets\n","# train_X, val_X, train_y, val_y = train_test_split(train_features, train_labels)\n","\n","# # Define individual models\n","# model_rf = RandomForestClassifier(n_estimators=100, random_state=42)\n","# model_gb = GradientBoostingClassifier(n_estimators=100, random_state=42)\n","# model_xgb = XGBClassifier(\n","#     n_estimators = 800,\n","#     max_depth = 13,\n","#     learning_rate = 0.08,\n","#     gamma = 0.5,\n","#     reg_lambda = 10,\n","#     min_child_weight = 7,\n","#     # objective = \"reg:squaredlogerror\"\n","#     # colsample_bytree = 0.45\n","#     # eval_metric = \"logloss\"\n","#     scale_pos_weight = 1\n","# )\n","\n","# # Define the stacking ensemble model\n","# estimators = [('rf', model_rf), ('gb', model_gb), ('xgb', model_xgb)]\n","# stacking_model = StackingClassifier(estimators=estimators, final_estimator=RandomForestClassifier(random_state=42))\n","\n","# # Train the stacking ensemble model\n","# stacking_model.fit(train_features, train_labels)\n","\n","# # Make predictions on the test set\n","# stacking_pred = stacking_model.predict(test_features)\n","\n","# # Calculate accuracy of the stacking ensemble model\n","# stacking_model.fit(train_X,train_y)\n","# pred_y = stacking_model.predict(val_X) \n","# stacking_accuracy = accuracy_score(val_y, pred_y)\n","# print(f\"Stacking Ensemble Model Accuracy: {stacking_accuracy}\")\n","\n","# -------------------------------------------------------\n","# Stacking Ensemble Model Accuracy: 0.9174216343540497\n","# -------------------------------------------------------"]},{"cell_type":"markdown","metadata":{},"source":["**3.  Stacking (logistic_regression, decision_tree, knn)**\n","\n","**-- Stacking Ensemble Model Accuracy: 0.8731880775502808**"]},{"cell_type":"code","execution_count":165,"metadata":{"execution":{"iopub.execute_input":"2023-12-16T15:10:02.698797Z","iopub.status.busy":"2023-12-16T15:10:02.697997Z","iopub.status.idle":"2023-12-16T15:10:02.717317Z","shell.execute_reply":"2023-12-16T15:10:02.715467Z","shell.execute_reply.started":"2023-12-16T15:10:02.698759Z"},"trusted":true},"outputs":[],"source":["# # Import necessary libraries\n","# from sklearn.preprocessing import StandardScaler\n","# from sklearn.model_selection import train_test_split\n","# from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n","# from sklearn.svm import SVC\n","# from sklearn.ensemble import StackingClassifier\n","# from sklearn.metrics import accuracy_score\n","# from sklearn.datasets import make_classification\n","# from xgboost import XGBClassifier\n","# from sklearn.linear_model import LogisticRegression\n","# from sklearn.tree import DecisionTreeClassifier\n","# from sklearn.neighbors import KNeighborsClassifier\n","# from sklearn.pipeline import make_pipeline\n","\n","# # Generate a toy dataset\n","# # X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n","\n","# # Split the dataset into training and testing sets\n","# train_X, val_X, train_y, val_y = train_test_split(train_features, train_labels)\n","\n","# # Define individual models\n","# model_lr = LogisticRegression(random_state=42)\n","# model_dt = DecisionTreeClassifier(random_state=42)\n","# model_knn = KNeighborsClassifier(n_neighbors=5)\n","\n","\n","# # Create a pipeline with StandardScaler and the models\n","# pipeline_dt = make_pipeline(StandardScaler(), model_dt)\n","# pipeline_lr = make_pipeline(StandardScaler(), model_lr)\n","# pipeline_knn = make_pipeline(StandardScaler(), model_knn)\n","\n","# # Define the stacking ensemble model\n","# estimators = [('lr', pipeline_lr), ('dt', pipeline_dt), ('knn', pipeline_knn)]\n","# stacking_model = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(random_state=42))\n","\n","\n","# # Train the stacking ensemble model\n","# stacking_model.fit(train_features, train_labels)\n","\n","# # Make predictions on the test set\n","# stacking_pred = stacking_model.predict(test_features)\n","\n","# # Calculate accuracy of the stacking ensemble model\n","# stacking_model.fit(train_X,train_y)\n","# pred_y = stacking_model.predict(val_X) \n","# stacking_accuracy = accuracy_score(val_y, pred_y)\n","# print(f\"Stacking Ensemble Model Accuracy: {stacking_accuracy}\")\n","\n","# ---------------------------------------------------------\n","# Stacking Ensemble Model Accuracy: 0.8731880775502808\n","# --------------------------------------------------------"]},{"cell_type":"markdown","metadata":{},"source":["**4. Stacking (random_forest, decision_tree, gradiant_boost), --With TPOT genetic algorithm for hyperparameter tuning--**\n","\n","**-- f1 score: 0.63244 (Kaggle)**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# from tpot import TPOTClassifier\n","# from sklearn.model_selection import train_test_split\n","# from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n","# from sklearn.tree import DecisionTreeClassifier\n","# from sklearn.linear_model import LogisticRegression\n","# from sklearn.metrics import accuracy_score\n","# from sklearn.datasets import make_classification\n","# from sklearn.ensemble import StackingClassifier\n","\n","# # Generate a toy dataset\n","# # X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n","\n","# # Split the dataset into training and testing sets\n","# train_X, val_X, train_y, val_y = train_test_split(train_features, train_labels)\n","\n","# base_models = [\n","#     ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n","#     ('dt', DecisionTreeClassifier(random_state=42)),\n","#     ('gb', GradientBoostingClassifier(n_estimators=100, random_state=42))\n","# ]\n","\n","# # Define the stacking ensemble model with a logistic regression meta-learner\n","# stacking_model = StackingClassifier(estimators=base_models, final_estimator=LogisticRegression())\n","\n","# # Define the TPOT genetic algorithm for hyperparameter tuning\n","# tpot = TPOTClassifier(\n","#     generations=5,\n","#     population_size=20,\n","#     random_state=42,\n","#     verbosity=2,\n","#     scoring='accuracy',\n","#     n_jobs=-1,\n","#     config_dict='TPOT sparse'\n","# )\n","\n","\n","\n","# # Fit the TPOT classifier (includes base models and stacking model hyperparameter optimization)\n","# tpot.fit(train_features, train_labels)\n","\n","# # Get the best pipeline found by TPOT\n","# best_pipeline = tpot.fitted_pipeline_\n","\n","# # Make predictions on the test set using the best pipeline\n","# ensemble_pred = best_pipeline.predict(test_features)\n","\n","# # Fit the TPOT classifier (includes base models and stacking model hyperparameter optimization)\n","# tpot.fit(train_X, train_y)\n","\n","# # Get the best pipeline found by TPOT\n","# best_pipeline = tpot.fitted_pipeline_\n","\n","# # Make predictions on the test set using the best pipeline\n","# pred_y = best_pipeline.predict(val_X)\n","\n","# # Evaluate the stacking ensemble model\n","# ensemble_accuracy = accuracy_score(val_y, pred_y)\n","# print(\"Stacking Ensemble Model Accuracy:\", pred_y)\n"]},{"cell_type":"markdown","metadata":{},"source":["<h2> Voting </h2>"]},{"cell_type":"markdown","metadata":{},"source":[" **1. Voting (random_forest, gradient_boosting, xgboost), --With XGB hyperparameters tuned--**\n"," \n"," **-- Voting Ensemble Model Accuracy: 0.9140695778220692**"]},{"cell_type":"code","execution_count":166,"metadata":{"execution":{"iopub.execute_input":"2023-12-16T15:10:02.719695Z","iopub.status.busy":"2023-12-16T15:10:02.719143Z","iopub.status.idle":"2023-12-16T15:10:02.737654Z","shell.execute_reply":"2023-12-16T15:10:02.735680Z","shell.execute_reply.started":"2023-12-16T15:10:02.719641Z"},"trusted":true},"outputs":[],"source":["# # Import necessary libraries\n","# from sklearn.model_selection import train_test_split\n","# from sklearn.ensemble import VotingClassifier\n","# from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n","# from sklearn.svm import SVC\n","# from sklearn.ensemble import StackingClassifier\n","# from sklearn.metrics import accuracy_score\n","# from sklearn.datasets import make_classification\n","# from xgboost import XGBClassifier\n","\n","# # Generate a toy dataset\n","# # X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n","\n","# # Split the dataset into training and testing sets\n","# train_X, val_X, train_y, val_y = train_test_split(train_features, train_labels)\n","\n","# # Define individual models\n","# model_rf = RandomForestClassifier(n_estimators=100, random_state=42)\n","# model_gb = GradientBoostingClassifier(n_estimators=100, random_state=42)\n","# model_xgb = XGBClassifier(\n","#     n_estimators = 800,\n","#     max_depth = 13,\n","#     learning_rate = 0.08,\n","#     gamma = 0.5,\n","#     reg_lambda = 10,\n","#     min_child_weight = 7,\n","#     # objective = \"reg:squaredlogerror\"\n","#     # colsample_bytree = 0.45\n","#     # eval_metric = \"logloss\"\n","#     scale_pos_weight = 1\n","# )\n","\n","# # Define the stacking ensemble model\n","# voting_model = VotingClassifier(estimators=[('rf', model_rf), ('gb', model_gb), ('xgb', model_xgb)], voting='hard')\n","\n","# # Train the stacking ensemble model\n","# voting_model.fit(train_features, train_labels)\n","\n","# # Make predictions on the test set\n","# voting_pred = stacking_model.predict(test_features)\n","\n","# # Calculate accuracy of the stacking ensemble model\n","# voting_model.fit(train_X,train_y)\n","# pred_y = voting_model.predict(val_X) \n","# voting_accuracy = accuracy_score(val_y, pred_y)\n","# print(f\"Voting Ensemble Model Accuracy: {voting_accuracy}\")\n","# ---------------------------------------------------------\n","# Voting Ensemble Model Accuracy: 0.9140695778220692\n","# ---------------------------------------------------------"]},{"cell_type":"markdown","metadata":{},"source":["**2. Voting (logistic_regression, decision_tree, knn),-- With Hyperparameters tuned, RandamizedSearchCV--**\n","\n","**-- f1 score: 0.63325 (Kaggle)**"]},{"cell_type":"code","execution_count":167,"metadata":{"execution":{"iopub.execute_input":"2023-12-16T15:10:02.742942Z","iopub.status.busy":"2023-12-16T15:10:02.741761Z","iopub.status.idle":"2023-12-16T15:10:02.758120Z","shell.execute_reply":"2023-12-16T15:10:02.756331Z","shell.execute_reply.started":"2023-12-16T15:10:02.742881Z"},"trusted":true},"outputs":[],"source":["# # Import necessary libraries\n","# from sklearn.model_selection import train_test_split, RandomizedSearchCV\n","# from sklearn.datasets import make_classification\n","# from sklearn.ensemble import VotingClassifier\n","# from sklearn.linear_model import LogisticRegression\n","# from sklearn.tree import DecisionTreeClassifier\n","# from sklearn.neighbors import KNeighborsClassifier\n","# from sklearn.ensemble import StackingClassifier\n","# from sklearn.metrics import accuracy_score\n","# from sklearn.datasets import make_classification\n","# from xgboost import XGBClassifier\n","# from sklearn.pipeline import make_pipeline\n","# from sklearn.preprocessing import StandardScaler\n","\n","# # Generate a toy dataset\n","# # X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n","\n","# # Split the dataset into training and testing sets\n","# train_X, val_X, train_y, val_y = train_test_split(train_features, train_labels)\n","\n","# # Define individual models with pipelines\n","# model_lr = make_pipeline(StandardScaler(), LogisticRegression(random_state=42))\n","# model_dt = make_pipeline(StandardScaler(), DecisionTreeClassifier(random_state=42))\n","# model_knn = make_pipeline(StandardScaler(), KNeighborsClassifier())\n","\n","# # Define the voting ensemble model\n","# ensemble_model = VotingClassifier(estimators=[('lr', model_lr), ('dt', model_dt), ('knn', model_knn)], voting='hard')\n","\n","# # Define hyperparameters for each model\n","# param_dist = {\n","#     'lr__logisticregression__C': np.logspace(-3, 3, 7),\n","#     'dt__decisiontreeclassifier__max_depth': [None] + list(np.arange(10, 31, 5)),\n","#     'knn__kneighborsclassifier__n_neighbors': [3, 5, 7, 9],\n","# }\n","\n","\n","\n","# # Use RandomizedSearchCV for hyperparameter tuning\n","# random_search = RandomizedSearchCV(estimator=ensemble_model, param_distributions=param_dist, scoring='f1', n_iter=10, cv=5)\n","# random_search.fit(train_features, train_labels)\n","\n","# # Get the best model from hyperparameter tuning\n","# best_model = random_search.best_estimator_\n","\n","# # Make predictions on the test set\n","# ensemble_pred = best_model.predict(test_features)\n","\n","# Calculate accuracy of the ensemble model\n","# ensemble_accuracy = accuracy_score(y_test, ensemble_pred)\n","# print(f\"Voting Ensemble Model Accuracy: {ensemble_accuracy}\")"]},{"cell_type":"markdown","metadata":{},"source":["**3. Voting (rf, dt, gb, ab, lr)**\n","\n","**-- Random Forest Accuracy: 0.87925801775684**\n","\n","**-- Decision Tree Accuracy: 0.8053768798695415**\n","\n","**-- Logistic Regression Accuracy: 0.7884807030259104**\n","\n","**-- AdaBoost Accuracy: 0.8020474723681826**\n","\n","**-- Gradient Boosting Accuracy: 0.8193060337017576**\n","\n","**-- Voting Classifier Accuracy: 0.8319668418191701**"]},{"cell_type":"code","execution_count":168,"metadata":{"execution":{"iopub.execute_input":"2023-12-16T15:10:02.760655Z","iopub.status.busy":"2023-12-16T15:10:02.759885Z","iopub.status.idle":"2023-12-16T15:38:11.242896Z","shell.execute_reply":"2023-12-16T15:38:11.240833Z","shell.execute_reply.started":"2023-12-16T15:10:02.760561Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_80/2820318235.py:31: DataConversionWarning:\n","\n","A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","\n","/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning:\n","\n","A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","\n","/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning:\n","\n","A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","\n","/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:437: DataConversionWarning:\n","\n","A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","\n","/opt/conda/lib/python3.10/site-packages/sklearn/preprocessing/_label.py:99: DataConversionWarning:\n","\n","A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","\n","/opt/conda/lib/python3.10/site-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning:\n","\n","A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","\n","/tmp/ipykernel_80/2820318235.py:49: DataConversionWarning:\n","\n","A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","\n","/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning:\n","\n","A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","\n","/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning:\n","\n","A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","\n","/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:437: DataConversionWarning:\n","\n","A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","\n","/opt/conda/lib/python3.10/site-packages/sklearn/preprocessing/_label.py:99: DataConversionWarning:\n","\n","A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","\n","/opt/conda/lib/python3.10/site-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning:\n","\n","A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","\n"]},{"name":"stdout","output_type":"stream","text":["Random Forest Accuracy: 0.87925801775684\n","Decision Tree Accuracy: 0.8053768798695415\n","Logistic Regression Accuracy: 0.7884807030259104\n","AdaBoost Accuracy: 0.8020474723681826\n","Gradient Boosting Accuracy: 0.8193060337017576\n","Voting Classifier Accuracy: 0.8319668418191701\n"]}],"source":["from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","from sklearn.datasets import make_classification\n","\n","# Generate a synthetic dataset\n","X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n","\n","# Split the dataset into training and testing sets\n","train_X, val_X, train_y, val_y = train_test_split(train_features, train_labels)\n","\n","# Base Models\n","rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n","dt_model = DecisionTreeClassifier(random_state=42)\n","lr_model = LogisticRegression(random_state=42)\n","ab_model = AdaBoostClassifier(n_estimators=50, random_state=42)\n","gb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n","\n","# Ensemble models\n","voting_model = VotingClassifier(estimators=[\n","    ('Random Forest', rf_model),\n","    ('Decision Tree', dt_model),\n","    ('Logistic Regression', lr_model),\n","    ('AdaBoost', ab_model),\n","    ('Gradient Boosting', gb_model)\n","], voting='hard')\n","\n","# Train each model separately\n","rf_model.fit(train_features, train_labels)\n","dt_model.fit(train_features, train_labels)\n","lr_model.fit(train_features, train_labels)\n","ab_model.fit(train_features, train_labels)\n","gb_model.fit(train_features, train_labels)\n","\n","# Voting Classifier\n","voting_model.fit(train_features, train_labels)\n","\n","# Make predictions\n","rf_pred = rf_model.predict(test_features)\n","dt_pred = dt_model.predict(test_features)\n","lr_pred = lr_model.predict(test_features)\n","ab_pred = ab_model.predict(test_features)\n","gb_pred = gb_model.predict(test_features)\n","voting_pred = voting_model.predict(test_features)\n","\n","# Train each model separately\n","rf_model.fit(train_X, train_y)\n","dt_model.fit(train_X, train_y)\n","lr_model.fit(train_X, train_y)\n","ab_model.fit(train_X, train_y)\n","gb_model.fit(train_X, train_y)\n","\n","# Voting Classifier\n","voting_model.fit(train_X, train_y)\n","\n","# Make predictions\n","rf_pred1 = rf_model.predict(val_X)\n","dt_pred1 = dt_model.predict(val_X)\n","lr_pred1 = lr_model.predict(val_X)\n","ab_pred1 = ab_model.predict(val_X)\n","gb_pred1 = gb_model.predict(val_X)\n","voting_pred1 = voting_model.predict(val_X)\n","\n","# Individual model accuracies\n","print(f\"Random Forest Accuracy: {accuracy_score(val_y, rf_pred1)}\")\n","print(f\"Decision Tree Accuracy: {accuracy_score(val_y, dt_pred1)}\")\n","print(f\"Logistic Regression Accuracy: {accuracy_score(val_y, lr_pred1)}\")\n","print(f\"AdaBoost Accuracy: {accuracy_score(val_y, ab_pred1)}\")\n","print(f\"Gradient Boosting Accuracy: {accuracy_score(val_y, gb_pred1)}\")\n","\n","# Ensemble model accuracy\n","print(f\"Voting Classifier Accuracy: {accuracy_score(val_y, voting_pred1)}\")\n"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":3847864,"sourceId":6668366,"sourceType":"datasetVersion"},{"datasetId":3847944,"sourceId":6668523,"sourceType":"datasetVersion"}],"dockerImageVersionId":30558,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
